{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs36 \cf0 LAB 7: artificial neural networks, support vector machines\

\fs28 Bayley Millar       ID:2940325\
\
SPIRAL
\b0 : \

\f1\fs24 \expnd0\expndtw0\kerning0
nn_model = MLP.MLP(layer_sizes=np.array([2, 50, 100, 1]),f_activation='relu',f_output='logsig', sigma=2.5)\

\f0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\sl280\partightenfactor0

\f1 \cf0 \expnd0\expndtw0\kerning0
nn_model.train(dset.x,dset.target_y,num_epochs=2000,alpha=0.01)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \
basically I used a large amount of layers because the problem is so complex. Without a small amount of layers i found that i was getting no good results. The other way to do it would be have a lot of different starting inputs with one hidden layer. The reason why I have a larger sigma is because with the starting weights being larger, the network found it easier to find the nessh.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2016-04-20 at 3.49.35 PM.png \width12400 \height9560
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs28 \cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 IRIS:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 PART1
\b \

\b0 \
\pard\pardeftab720\sl280\partightenfactor0

\f1\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 nn_model = MLP.MLP(layer_sizes=np.array([2, 5, 5, 3]),f_activation='relu',f_output='softmax', sigma=1.5)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\pardeftab720\sl280\partightenfactor0

\f1\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 nn_model.train(dset.x,dset.target_y,num_epochs=1000,alpha=0.05)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2016-04-27 at 1.34.49 PM.png \width12680 \height10200
}¬}
\fs28 \
\
PART 2\
\
Kept the same architecture as part 1.\
training classification error 5%\
test classification error 0%\
\
The architecture was the same from the 2-d iris to the 4d. It performed much better as I had lower error percentages. I believe this is because as there is more information given to the network it does a better job with the data.\
\

\b \
\
DIGITS:\
\
\pard\pardeftab720\sl280\partightenfactor0

\f1\b0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 nn_model = MLP.MLP(layer_sizes=np.array([64, 10, 10, 5]),f_activation='relu',f_output='softmax', sigma=1.5)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\pardeftab720\sl280\partightenfactor0

\f1\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 nn_model.train(dset.x,dset.target_y,num_epochs=1000,alpha=0.1)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
Best training error i got was 1.388889%\
best test error i got was 5.524862\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2016-04-27 at 1.50.23 PM.png \width14040 \height10980
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs28 \cf0 \
\
\
\
\
}